# Seq2Seq-Attention-Model
This is a seq2seq with attention model for language translation task. A simple neural machine translation model is created

## components:
- Bidrectional LSTM Encoder
- Unidirection LSTM Decoder
- Global Attention Model (Luong, et al. 2015)

## Requirements:
numpy
pandas
nltk
torch
ipykernel
unidecode
sklearn
matplotlib
seaborn